{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fece424",
   "metadata": {},
   "source": [
    "## Lazy Fox Ground Truth Metrics\n",
    "All datasets we worked with provide ground truth community data and we can therefore compute scores such as F1 and NMI.\n",
    "\n",
    "However, the provided ground truths are not directly usable for comparision as a) they are not provided in a format readable by networkit and b) LazyFox' output is neither.\n",
    "\n",
    "We need to rewrite the ground truth and the LazyFox cluster output.\n",
    "\n",
    "Note also that any comparision to ground truth in this field will yield very low scores, as there is no definition of the term \"community\". See our project report for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad095f9",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c55eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"./datasets\"\n",
    "# Where the converted datasets and ground truths are saved\n",
    "output_directory = dataset_directory\n",
    "\n",
    "# What dataset is used\n",
    "dataset = \"eu\"\n",
    "# What LazyFox run to compare\n",
    "run_directory = \"./example_data/run_eu_with_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045bfc4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This section downloads and rewrites the specified dataset, if not already present.\n",
    "\n",
    "It also rewrites the final cluster result of the specified run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494388a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets import download\n",
    "from Rewriter import Rewriter\n",
    "from BenchmarkRun import BenchmarkRun\n",
    "\n",
    "download(dataset, dataset_directory)\n",
    "\n",
    "rewriter = Rewriter(dataset_directory, output_directory)\n",
    "rewriter.rewrite_dataset(dataset)\n",
    "\n",
    "run = BenchmarkRun(dataset, run_directory)\n",
    "rewriter.rewrite_lazyfox_result(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d229e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(dataset):\n",
    "    return join(output_directory, f\"rewritten_{dataset}_gt.txt\")\n",
    "\n",
    "def get_graph(dataset):\n",
    "    return join(output_directory, f\"rewritten_{dataset}_graph.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit.community import CoverF1Similarity, OverlappingNMIDistance\n",
    "\n",
    "def calc_f1(graph, ground_truth, lazy_fox_result):\n",
    "    # F1\n",
    "    f1 = CoverF1Similarity(graph, ground_truth, lazy_fox_result)\n",
    "    f1.run()\n",
    "    return f1.getWeightedAverage()\n",
    "\n",
    "def calc_nmi(graph, ground_truth, lazy_fox_result):\n",
    "    nmi = OverlappingNMIDistance()\n",
    "    distance = nmi.getDissimilarity(graph, ground_truth, lazy_fox_result)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96940d60",
   "metadata": {},
   "source": [
    "## Ground Truth Analysis\n",
    "Finally, we can compute the F1 and the NMI score of the specified run.\n",
    "\n",
    "Note that this computation can take very long, dependent on graph scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Runs analysis for one specific run for one specific iteration (default: final iteration)\"\"\"\n",
    "\n",
    "from networkit.graphio import CoverReader\n",
    "import networkit as nk\n",
    "from os.path import join\n",
    "from pandas import DataFrame\n",
    "\n",
    "print(dataset)\n",
    "print(\"Loading Graph\")\n",
    "graph = nk.readGraph(get_graph(dataset), nk.Format.SNAP)\n",
    "cover_reader = CoverReader()\n",
    "print(\"Loading GT\")\n",
    "ground_truth = cover_reader.read(get_ground_truth(dataset), graph)\n",
    "\n",
    "iteration = len(run.iterations) - 1\n",
    "\n",
    "lazy_fox_result_path = join(run.run_directory, \"iterations\", f\"rewritten_{iteration}clusters.txt\")\n",
    "lazy_fox_result = cover_reader.read(lazy_fox_result_path, graph)\n",
    "\n",
    "print(\"Computing F1\")\n",
    "f1 = calc_f1(graph, ground_truth, lazy_fox_result)\n",
    "print(f\"wei. avg: {f1}\")\n",
    "print(\"Computing NMI\")\n",
    "nmi_dis = calc_nmi(graph, ground_truth, lazy_fox_result)\n",
    "print(f\"nmi: {nmi_dis}\")\n",
    "\n",
    "df = DataFrame([[dataset, f1, nmi_dis]], columns=[\"Dataset\", \"F1\", \"NMI Distance\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
